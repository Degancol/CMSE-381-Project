{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FiczPXhqA35T"
   },
   "source": [
    "# CMSE 381 Final Project Template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o6HR6z_BA35W"
   },
   "source": [
    "**INSTRUCTIONS**: This is a template to help organize your project.  All projects should include the 5 major sections below (you do not need to use this template file).  If you use this file, complete your work below and remove content in parentheses. Also, remove this current cell.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vQ-ZENBIA35W"
   },
   "source": [
    "#### CMSE 381 Final Project\n",
    "### &#9989; Group members: Colby Degan, Jose Villegas\n",
    "### &#9989; Section_002\n",
    "#### &#9989; 4/25/2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8fBUG7ctA35X"
   },
   "source": [
    "# Concrete Crack Detection using SVCs and CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h9RGCASwA35X"
   },
   "source": [
    "## Background and Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D1b4LSXwA35X"
   },
   "source": [
    " When it comes to classification problems in Data Science and Machine Learning, the utility of images can often be very useful to do so. For this particular project, a large directory of images was provided, within the directory there were two sub-directories split into categories of Cracked, and Non-cracked, then within those directories there were nested directories that consisted of the type, or structure of concrete, which were of the following: pavement, deck, and wall. Our overall goal was to consider two machine learning processes/algorithms that we learned in class, and apply those models to these different kinds of images to make a model that can optimally classify whether or not images are cracked. After these models are created, these models will be optimized utilizing some form of cross validation to make improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TxsOyMX_A35Y"
   },
   "source": [
    "## Methodology\n",
    "There were two approaches that we utilized to create our models, the first of which was an Support Vector Classifier that was run on each of the three different structures, then the optimization approach was done using the GridSearchCV class from Sklearn. the other approach was done thorough A Convolutional neural network. Which is a neural network that excels in image classification, and attempts to mimic similar classification approaches that humans make. The optimization to the CNN will be to utilize gridsearch or apply dropout regularization to optimize the CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "n2VxM5yXA35Y"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "from torch.utils.data import random_split\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IwryzfCgA35Z",
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Data\n",
    "_(Describe the data you are using. What variables are you using? What they mean? Why did you choose them?)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6rR0xioFA35Z"
   },
   "source": [
    "We are compressing each 256x256 image into 64x64, and are using each pixel measurement of that compressed image as a variable, giving us 4096 pixel measurements. We are also using the type of structure as a variable, as it wouldn't be logical to compare a picture of cracked pavement to a picture of an un-cracked wall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IJiPRlnGA35a"
   },
   "outputs": [],
   "source": [
    "\n",
    "folder_paths = [\"archive/Decks/Cracked\", \"archive/Decks/Non-cracked\", \"archive/Pavements/Cracked\", \"archive/Pavements/Non-cracked\", \"archive/Walls/Cracked\", \"archive/Walls/Non-cracked\"]\n",
    "image_data = []\n",
    "cracked = []\n",
    "structure = []\n",
    "\n",
    "# For every folder that contains images\n",
    "for folder_path in folder_paths:\n",
    "\n",
    "    # Get list of image files\n",
    "    image_files = [f for f in os.listdir(folder_path) if f.endswith('.jpg')]\n",
    "\n",
    "    # Initialize list to store image data\n",
    "    for i in range(len(image_files)):\n",
    "        img_path = os.path.join(folder_path, image_files[i])\n",
    "        img = Image.open(img_path).convert('L')\n",
    "        img.thumbnail((16, 16), Image.Resampling.LANCZOS)\n",
    "        img_array = np.array(img).flatten()\n",
    "        image_data.append(img_array)\n",
    "        if folder_path == \"archive/Decks/Cracked\" or folder_path == \"archive/Decks/Non-cracked\":\n",
    "            structure.append(\"Deck\")\n",
    "        elif folder_path == \"archive/Pavements/Cracked\" or folder_path == \"archive/Pavements/Non-cracked\":\n",
    "            structure.append(\"Pavement\")\n",
    "        else:\n",
    "            structure.append(\"Wall\")\n",
    "        if folder_path == \"archive/Decks/Cracked\" or folder_path == \"archive/Pavements/Cracked\" or folder_path == \"archive/Walls/Cracked\":\n",
    "            cracked.append(1)\n",
    "        else:\n",
    "            cracked.append(0)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(image_data)\n",
    "df[\"Structure\"] = structure\n",
    "df[\"Cracked\"] = cracked\n",
    "df.to_csv(\"concrete_dataset_16.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcreteDataset(Dataset):\n",
    "    \"\"\" Object set to take in images from the provided dataset\n",
    "\n",
    "        The reason for this Class is to be able to utilize DataLoaders for the CNN model, \n",
    "        two different approaches were used to read in the data depending on the model being used.\n",
    "    \"\"\"\n",
    "    def __init__(self, folder_paths, transform=None):\n",
    "        \"\"\" Initialization function consisting of:\n",
    "\n",
    "        - image data: \n",
    "        - Cracked: list for all cracked images (1) or non-cracked (0)\n",
    "        - structure: list for structure types in concrete (wall, pavement, decks)\n",
    "        \"\"\"\n",
    "        self.image_data = []\n",
    "        self.cracked = []\n",
    "        self.structure = []\n",
    "        self.transform = transform\n",
    "        \n",
    "        # For each folder (cracked or non-cracked)\n",
    "        for folder_path in folder_paths:\n",
    "            # Get list of image files\n",
    "            image_files = [f for f in os.listdir(folder_path) if f.endswith('.jpg')]\n",
    "\n",
    "            for filename in image_files:\n",
    "                img_path = os.path.join(folder_path, filename)\n",
    "                img = Image.open(img_path).convert('L')  # Grayscale image\n",
    "                img.thumbnail((16, 16), Image.Resampling.LANCZOS)\n",
    "                img_array = np.array(img)  # Keep as 2D array\n",
    "\n",
    "                # Pad to exactly 64x64 if needed\n",
    "                if img_array.shape != (16, 16):\n",
    "                    padded_img = np.zeros((16, 16), dtype=np.uint8)\n",
    "                    padded_img[:img_array.shape[0], :img_array.shape[1]] = img_array\n",
    "                    img_array = padded_img\n",
    "\n",
    "                # Append image data\n",
    "                self.image_data.append(img_array)\n",
    "                \n",
    "                # Append structure type\n",
    "                self.structure.append(folder_path.split('/')[0])  # Extract structure (Deck, Pavement, Wall)\n",
    "                \n",
    "                # Append cracked/non-cracked label\n",
    "                if 'Cracked' in folder_path:\n",
    "                    self.cracked.append(1)\n",
    "                else:\n",
    "                    self.cracked.append(0)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\" Helpful length function to get size of Dataset Easily \"\"\"\n",
    "        return len(self.image_data)\n",
    "                    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\" This function allows for easy access of the data's  image at a particular index\n",
    "            and the images' respective label, 0 representing Cracked, and 1 representing Non-Cracked\n",
    "        \"\"\"\n",
    "        image = self.image_data[idx]\n",
    "        label = self.cracked[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_paths = [\n",
    "    \"Decks/Cracked\", \"Decks/Non-cracked\", \n",
    "    \"Pavements/Cracked\", \"Pavements/Non-cracked\", \n",
    "    \"Walls/Cracked\", \"Walls/Non-cracked\"\n",
    "]\n",
    "image_data = []\n",
    "cracked = []\n",
    "structure = []\n",
    "\n",
    "# Creating a transformation pipeline which normalizes images to get into correct range\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert image to PyTorch tensor\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize to [0, 1] range\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Concrete_Images = ConcreteDataset(folder_paths, transform=transform)\n",
    "\n",
    "train_size = int(0.8 * len(Concrete_Images))  # 80% for training\n",
    "test_size = len(Concrete_Images) - train_size\n",
    "train_dataset, test_dataset = random_split(Concrete_Images, [train_size, test_size])\n",
    "\n",
    "# Utilizing dataloaders in order to process batches of images instead of attempting to do it all at once\n",
    "# this makes things less computationally expensive\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "id": "ng2P3tl7A35a",
    "outputId": "9dda09d9-c107-4362-fae5-16d1a1b2bdd1"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "dataframe",
       "variable_name": "concrete_dataset"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-2494552d-d650-4a54-84de-56fc64165ef7\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "      <th>Structure</th>\n",
       "      <th>Cracked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>179</td>\n",
       "      <td>172</td>\n",
       "      <td>172</td>\n",
       "      <td>179</td>\n",
       "      <td>177</td>\n",
       "      <td>180</td>\n",
       "      <td>181</td>\n",
       "      <td>179</td>\n",
       "      <td>175</td>\n",
       "      <td>178</td>\n",
       "      <td>...</td>\n",
       "      <td>177</td>\n",
       "      <td>181</td>\n",
       "      <td>185</td>\n",
       "      <td>183</td>\n",
       "      <td>186</td>\n",
       "      <td>191</td>\n",
       "      <td>183</td>\n",
       "      <td>181</td>\n",
       "      <td>Deck</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>181</td>\n",
       "      <td>182</td>\n",
       "      <td>176</td>\n",
       "      <td>178</td>\n",
       "      <td>180</td>\n",
       "      <td>177</td>\n",
       "      <td>180</td>\n",
       "      <td>179</td>\n",
       "      <td>181</td>\n",
       "      <td>186</td>\n",
       "      <td>...</td>\n",
       "      <td>185</td>\n",
       "      <td>185</td>\n",
       "      <td>180</td>\n",
       "      <td>179</td>\n",
       "      <td>186</td>\n",
       "      <td>177</td>\n",
       "      <td>182</td>\n",
       "      <td>180</td>\n",
       "      <td>Deck</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>182</td>\n",
       "      <td>188</td>\n",
       "      <td>187</td>\n",
       "      <td>196</td>\n",
       "      <td>193</td>\n",
       "      <td>190</td>\n",
       "      <td>185</td>\n",
       "      <td>185</td>\n",
       "      <td>188</td>\n",
       "      <td>188</td>\n",
       "      <td>...</td>\n",
       "      <td>189</td>\n",
       "      <td>187</td>\n",
       "      <td>189</td>\n",
       "      <td>191</td>\n",
       "      <td>186</td>\n",
       "      <td>192</td>\n",
       "      <td>190</td>\n",
       "      <td>189</td>\n",
       "      <td>Deck</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>199</td>\n",
       "      <td>194</td>\n",
       "      <td>176</td>\n",
       "      <td>175</td>\n",
       "      <td>186</td>\n",
       "      <td>186</td>\n",
       "      <td>184</td>\n",
       "      <td>184</td>\n",
       "      <td>179</td>\n",
       "      <td>170</td>\n",
       "      <td>...</td>\n",
       "      <td>181</td>\n",
       "      <td>177</td>\n",
       "      <td>182</td>\n",
       "      <td>178</td>\n",
       "      <td>178</td>\n",
       "      <td>181</td>\n",
       "      <td>179</td>\n",
       "      <td>177</td>\n",
       "      <td>Deck</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>176</td>\n",
       "      <td>180</td>\n",
       "      <td>179</td>\n",
       "      <td>182</td>\n",
       "      <td>177</td>\n",
       "      <td>179</td>\n",
       "      <td>177</td>\n",
       "      <td>178</td>\n",
       "      <td>178</td>\n",
       "      <td>181</td>\n",
       "      <td>...</td>\n",
       "      <td>180</td>\n",
       "      <td>175</td>\n",
       "      <td>174</td>\n",
       "      <td>180</td>\n",
       "      <td>179</td>\n",
       "      <td>178</td>\n",
       "      <td>181</td>\n",
       "      <td>182</td>\n",
       "      <td>Deck</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 258 columns</p>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2494552d-d650-4a54-84de-56fc64165ef7')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-2494552d-d650-4a54-84de-56fc64165ef7 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-2494552d-d650-4a54-84de-56fc64165ef7');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-f8d51435-a495-4ee8-a3a9-95cd8a5d86e5\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f8d51435-a495-4ee8-a3a9-95cd8a5d86e5')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-f8d51435-a495-4ee8-a3a9-95cd8a5d86e5 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "     0    1    2    3    4    5    6    7    8    9  ...  248  249  250  251  \\\n",
       "0  179  172  172  179  177  180  181  179  175  178  ...  177  181  185  183   \n",
       "1  181  182  176  178  180  177  180  179  181  186  ...  185  185  180  179   \n",
       "2  182  188  187  196  193  190  185  185  188  188  ...  189  187  189  191   \n",
       "3  199  194  176  175  186  186  184  184  179  170  ...  181  177  182  178   \n",
       "4  176  180  179  182  177  179  177  178  178  181  ...  180  175  174  180   \n",
       "\n",
       "   252  253  254  255  Structure  Cracked  \n",
       "0  186  191  183  181       Deck        1  \n",
       "1  186  177  182  180       Deck        1  \n",
       "2  186  192  190  189       Deck        1  \n",
       "3  178  181  179  177       Deck        1  \n",
       "4  179  178  181  182       Deck        1  \n",
       "\n",
       "[5 rows x 258 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concrete_dataset = pd.read_csv('concrete_dataset_16.csv')\n",
    "concrete_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 256
    },
    "id": "F9NogGfzA35a",
    "outputId": "01a1b3b5-e0cf-44be-fc28-36befd8e722f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "dataframe",
       "variable_name": "concrete_dataset"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-572cdce0-d8b5-4cca-8f43-fa26211f5f3b\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "      <th>Cracked</th>\n",
       "      <th>Structure_Pavement</th>\n",
       "      <th>Structure_Wall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>179</td>\n",
       "      <td>172</td>\n",
       "      <td>172</td>\n",
       "      <td>179</td>\n",
       "      <td>177</td>\n",
       "      <td>180</td>\n",
       "      <td>181</td>\n",
       "      <td>179</td>\n",
       "      <td>175</td>\n",
       "      <td>178</td>\n",
       "      <td>...</td>\n",
       "      <td>181</td>\n",
       "      <td>185</td>\n",
       "      <td>183</td>\n",
       "      <td>186</td>\n",
       "      <td>191</td>\n",
       "      <td>183</td>\n",
       "      <td>181</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>181</td>\n",
       "      <td>182</td>\n",
       "      <td>176</td>\n",
       "      <td>178</td>\n",
       "      <td>180</td>\n",
       "      <td>177</td>\n",
       "      <td>180</td>\n",
       "      <td>179</td>\n",
       "      <td>181</td>\n",
       "      <td>186</td>\n",
       "      <td>...</td>\n",
       "      <td>185</td>\n",
       "      <td>180</td>\n",
       "      <td>179</td>\n",
       "      <td>186</td>\n",
       "      <td>177</td>\n",
       "      <td>182</td>\n",
       "      <td>180</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>182</td>\n",
       "      <td>188</td>\n",
       "      <td>187</td>\n",
       "      <td>196</td>\n",
       "      <td>193</td>\n",
       "      <td>190</td>\n",
       "      <td>185</td>\n",
       "      <td>185</td>\n",
       "      <td>188</td>\n",
       "      <td>188</td>\n",
       "      <td>...</td>\n",
       "      <td>187</td>\n",
       "      <td>189</td>\n",
       "      <td>191</td>\n",
       "      <td>186</td>\n",
       "      <td>192</td>\n",
       "      <td>190</td>\n",
       "      <td>189</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>199</td>\n",
       "      <td>194</td>\n",
       "      <td>176</td>\n",
       "      <td>175</td>\n",
       "      <td>186</td>\n",
       "      <td>186</td>\n",
       "      <td>184</td>\n",
       "      <td>184</td>\n",
       "      <td>179</td>\n",
       "      <td>170</td>\n",
       "      <td>...</td>\n",
       "      <td>177</td>\n",
       "      <td>182</td>\n",
       "      <td>178</td>\n",
       "      <td>178</td>\n",
       "      <td>181</td>\n",
       "      <td>179</td>\n",
       "      <td>177</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>176</td>\n",
       "      <td>180</td>\n",
       "      <td>179</td>\n",
       "      <td>182</td>\n",
       "      <td>177</td>\n",
       "      <td>179</td>\n",
       "      <td>177</td>\n",
       "      <td>178</td>\n",
       "      <td>178</td>\n",
       "      <td>181</td>\n",
       "      <td>...</td>\n",
       "      <td>175</td>\n",
       "      <td>174</td>\n",
       "      <td>180</td>\n",
       "      <td>179</td>\n",
       "      <td>178</td>\n",
       "      <td>181</td>\n",
       "      <td>182</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 259 columns</p>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-572cdce0-d8b5-4cca-8f43-fa26211f5f3b')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-572cdce0-d8b5-4cca-8f43-fa26211f5f3b button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-572cdce0-d8b5-4cca-8f43-fa26211f5f3b');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-b7bd8969-4aee-43fa-b4b9-3e3bcb5fe4e3\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b7bd8969-4aee-43fa-b4b9-3e3bcb5fe4e3')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-b7bd8969-4aee-43fa-b4b9-3e3bcb5fe4e3 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "     0    1    2    3    4    5    6    7    8    9  ...  249  250  251  252  \\\n",
       "0  179  172  172  179  177  180  181  179  175  178  ...  181  185  183  186   \n",
       "1  181  182  176  178  180  177  180  179  181  186  ...  185  180  179  186   \n",
       "2  182  188  187  196  193  190  185  185  188  188  ...  187  189  191  186   \n",
       "3  199  194  176  175  186  186  184  184  179  170  ...  177  182  178  178   \n",
       "4  176  180  179  182  177  179  177  178  178  181  ...  175  174  180  179   \n",
       "\n",
       "   253  254  255  Cracked  Structure_Pavement  Structure_Wall  \n",
       "0  191  183  181        1               False           False  \n",
       "1  177  182  180        1               False           False  \n",
       "2  192  190  189        1               False           False  \n",
       "3  181  179  177        1               False           False  \n",
       "4  178  181  182        1               False           False  \n",
       "\n",
       "[5 rows x 259 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concrete_dataset = pd.get_dummies(concrete_dataset, drop_first = True)\n",
    "concrete_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qUGqeKqqA35a"
   },
   "source": [
    "### Models for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r5foDo2rA35b"
   },
   "source": [
    "#### SVC with GridSearchCV\n",
    "\n",
    "We chose this because we thought it would be useful for image classification. By using GridSearchCV we are able to check what the best possible parameters are for the model. We will evaluate this model by checking its training and testing errors to see how well the model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GSNTfCV1A35b",
    "outputId": "2406807f-182b-4dfa-f092-df163c9b1467"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "X = concrete_dataset.drop(columns = [\"Cracked\"])\n",
    "y = concrete_dataset.Cracked\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify = y, random_state = 42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "smote = SMOTE(random_state = 42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "svc = SVC(class_weight='balanced', random_state=42)\n",
    "\n",
    "param_grid = {'C': [0.1, 1, 10], 'kernel': ['rbf', 'linear'], 'gamma': ['scale', 'auto', 0.1]}\n",
    "\n",
    "grid_search = GridSearchCV(svc, param_grid, cv = 5, scoring = 'accuracy', n_jobs = -1)\n",
    "grid_search.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "best_svc = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L_EmP6g1A35b"
   },
   "source": [
    "#### CNN with Nested Loop Hyperparameter Search\n",
    "\n",
    "We chose this because we thought it would be useful to identify smaller changes in the model, which is useful for noticing small cracks. We were unable to properly use GridSearchCV for the CNN and Pytoarch and Sklearn are incompatible. To get around this we wrote a custom nested loop hyperparameter search to determine the best hyperparameters for the CNN. We will evaluate the CNN using accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tmo9UqILA35b"
   },
   "outputs": [],
   "source": [
    "CNN_model = nn.Sequential(\n",
    "    # This is the Convolutional Layer, with the single input being the image itself, however the image itself\n",
    "    # Is a 4 dimensional Tensor object, as this is how Pytorch CNNs are implemented\n",
    "    # The kernel size is representative of the matrix that actually does the convolving over the input\n",
    "    nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
    "    nn.ReLU(), # Using ReLU \n",
    "    nn.MaxPool2d(kernel_size=2, stride=2), # Pooling layer to find the maximum value to represent the 2x2 pooling sample of our image\n",
    "\n",
    "    # A hidden Convolutional layer reading the output of the previous (size 16) and using a similar kernel size to convolve over current image\n",
    "    nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "    nn.ReLU(), # ReLU once again as activation\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2), # Pooling layer used to similar find max of 2x2 pooling matrix\n",
    "\n",
    "    nn.Flatten(),  # Using flatten to turn 32, 16, 16 into a single value\n",
    "    nn.Linear(32 * 4 * 4, 64),\n",
    "    nn.ReLU(), # ReLU!\n",
    "    nn.Linear(64, 1)  # One output for binary classification\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch device is able to use GPU to run model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "CNN_model = CNN_model.to(device)\n",
    "\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(CNN_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "# Each epoch iteration\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # Training CNN\n",
    "    CNN_model.train()\n",
    "\n",
    "    # Loss variable to track total loss\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Now iterating through dataloader to limit image processing to make more managable\n",
    "    for images, labels in train_loader: \n",
    "\n",
    "        # Moving the images and labels to the same device as model for computations\n",
    "        images = images.to(device)\n",
    "        labels = labels.float().unsqueeze(1).to(device)\n",
    "\n",
    "        #\n",
    "        optimizer.zero_grad()\n",
    "        outputs = CNN_model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {running_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "all_predicts = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        predictions = torch.sigmoid(outputs) > 0.5\n",
    "        \n",
    "        all_predict.extend(predictions.cpu().numpy())\n",
    "        all_labelsextend(labels.cpu().numpy())\n",
    "        \n",
    "accuracy = sum(np.array(all_predicts) == np.array(all_labels)) / len(all_labels)\n",
    "print(f\"Accuracy: {100 * accuracy:.2f}%\")\n",
    "\n",
    "print(classification_report(all_labels, all_predicts, target_named=[\"Uncracked\", \"Cracked\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN with Manual Grid Search optimization\n",
    "\n",
    "The Benefit to using A grid seach validation method would be to find the most optimal selection of parameters, those that were considered were, optimizer, batch size, and learning rate. The reason this approach was used is because of one main issue, Sklearns gridsearch doesn't have a direct application with pytorch, and because of this, the product function is used from itertools to create matrix of different learning rates, batches, and optimizers to find an optimal model, this will run several CNNs over the course of what is likely a long time. The results were evaluated by getting the ratio of correctly classified images over the total number of images looked at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.01, 0.001]\n",
    "batches = [16, 32, 64]\n",
    "\n",
    "grid_iter = list(product(learning_rates, batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for lr, curr_batch, opt in grid_iter:\n",
    "    print(f\"\\n Current learning rate: {lr}, batch size: {curr_batch}, Optimizer: {opt}\")\n",
    "\n",
    "    \n",
    "    # The reason for making different loaders for every instance is due to the different batch sizes to find most optimal hyperparameter\n",
    "    curr_loader = DataLoader(Concrete_Images, batch_size = curr_batch, shuffle = True)\n",
    "\n",
    "\n",
    "    # Creating a model for the multiple iterations\n",
    "    curr_model = nn.Sequential(\n",
    "        nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2, 2),\n",
    "\n",
    "        nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2, 2),\n",
    "\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(32 * 16 * 16, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, 1)\n",
    "    ).to(device)\n",
    "\n",
    "    # Need to have specific conditions to use different optimizers when neccessary \n",
    "\n",
    "    optimizer = optim.Adam(curr_model.parameters(), lr=lr)\n",
    "\n",
    "    # Need our binary classifier using Binary classification log loss function\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Now we need to actually train our model(s) will be done with set amount of epochs (5)\n",
    "    for epoch in range(5):\n",
    "\n",
    "        # Call train function on the \n",
    "        curr_model.train()\n",
    "        constant_loss = 0.0\n",
    "\n",
    "        # Now iterate through the images and their respective layers through our DataLoader for Concrete data\n",
    "        for images, labels in curr_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.float().unsqueeze(1).to(device)\n",
    "\n",
    "            # Reset gradients so each model doesn't have incorrect gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Store outputs as model run on images\n",
    "            outputs = curr_model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Add to constant loss to get overall loss by end of model run\n",
    "            constant_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Loss: {constant_loss:.4f}\")\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    curr_model.eval()\n",
    "\n",
    "    # Code to get accuracy of particular model\n",
    "    with torch.no_grad():\n",
    "        for images, labels in curr_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.float().unsqueeze(1).to(device)\n",
    "    \n",
    "            outputs = curr_model(images)\n",
    "            predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    results.append({\n",
    "        'lr': lr,\n",
    "        'batch_size': curr_batch,\n",
    "        'optimizer': opt,\n",
    "        'accuracy': accuracy\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d05nhNuPA35b"
   },
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IeefecE1A35c"
   },
   "source": [
    "_(What did you find when you carried out your methods? Some of your code related to\n",
    "presenting results/figures/data may be replicated from the methods section or may only be present in\n",
    "this section. All of the plots that you plan on using for your presentation should be present in this\n",
    "section)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZHJ8Hq5aA35c"
   },
   "source": [
    "### SVC results\n",
    "_(What are you trying to do here?)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 753
    },
    "id": "86aEgOtGA35c",
    "outputId": "4f5d8aa3-6447-4068-e83f-21414943dec5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7828683483376415\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.88      0.87      9522\n",
      "           1       0.26      0.23      0.24      1697\n",
      "\n",
      "    accuracy                           0.78     11219\n",
      "   macro avg       0.56      0.56      0.56     11219\n",
      "weighted avg       0.77      0.78      0.78     11219\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[8394 1128]\n",
      " [1308  389]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7ad9a0dc3510>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "y_pred = best_svc.predict(X_test_scaled)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred)).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pGOViASJA35c"
   },
   "source": [
    "_(How do you interpret what you see?)_\n",
    "\n",
    "_(What are you doing next?)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lKJLLwAWA35c"
   },
   "outputs": [],
   "source": [
    "# how did you do it (etc. etc.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_hbG2zMVA35c"
   },
   "source": [
    "### CNN results\n",
    "\n",
    "The initial model had a learning rate of 0.001, using the Adam optimizer, and a batch size of 32\n",
    "\n",
    "This model didn’t perform that poorly, but a question of which hyperparameters were best was yet to be answered. Because of this the optimization process would then turn into a gridsearch process where multiple arrays of hyperparameters would be used to run multiple different models, because of the large amount of Neural Networks this would create, some things had to be toned down, like number of epochs for example. Along with this in order to reduce runtime, images were scaled down to 16x16 pixels. This may have led to some incorrect classifications but also resulted in quicker results. Unfortunately, the typical parameters used to assess neural networks, that being f1-score, precision, and recall were not programmed in before running the model, so going based off accuracy of classification is the only parameter being discussed.\n",
    "\n",
    "\n",
    "The initial model is on the left, which had an okay accuracy of 86%, with 10 epochs to run through. The optimized model(s) however have varying results, The model with the configuration Adam, learning rate 0.01, and batch size 64 had the greatest overall loss, whereas the SGD, 0.01, 64, configuration maintained consistency, but was slower than the Adam variation. A model that does not do very well is the model with optimizer SGD, learning rate 0.01, and batch size 16, as the loss drop is not a significant, which indicates that the model is not learning as well, this may be due to the small batch sizes not doing well with the faster learning rate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JlXb_iBuA35c"
   },
   "outputs": [],
   "source": [
    "results_16 = {\n",
    "    (\"SGD\", 0.01, 16): [1445.4743, 1441.6831, 1438.9502, 1437.0728, 1433.2455],\n",
    "    (\"Adam\", 0.01, 16): [1475.2849, 1460.9638, 1460.2559, 1459.9407, 1460.0902],\n",
    "    (\"SGD\", 0.001, 16): [1492.5712, 1444.7881, 1443.2383, 1442.3714, 1441.5324],\n",
    "    (\"Adam\", 0.001, 16): [1389.5200, 1279.5868, 1181.0295, 1164.8745, 1148.8653]\n",
    "}\n",
    "\n",
    "results_32 = {\n",
    "    (\"SGD\", 0.01, 32): [730.2184, 720.4164, 719.2340, 719.3720, 718.7629],\n",
    "    (\"Adam\", 0.01, 32): [713.6464, 670.1269, 645.0783, 616.9844, 592.5035]\n",
    "}\n",
    "\n",
    "results_64 = {\n",
    "    (\"SGD\", 0.01, 64): [366.5066, 360.8374, 359.9431, 359.1196, 358.6196],\n",
    "    (\"Adam\", 0.01, 64): [364.7134, 356.2679, 341.4328, 324.2015, 315.0432],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (opt, lr, bs), losses in results_16.items():\n",
    "    label = f\"{opt}, lr={lr}, bs={bs}\"\n",
    "    plt.plot(range(1, len(losses)+1), losses, label=label)\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss per Epoch for Different Hyperparameters')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (opt, lr, bs), losses in results_32.items():\n",
    "    label = f\"{opt}, lr={lr}, bs={bs}\"\n",
    "    plt.plot(range(1, len(losses)+1), losses, label=label)\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss per Epoch for different Hyperparameters\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (opt, lr, bs), losses in results_64.items():\n",
    "    label = f\"{opt}, lr={lr}, bs={bs}\"\n",
    "    plt.plot(range(1, len(losses)+1), losses, label=label)\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss per Epoch for different Hyperparameters\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hfqqnObIA35c"
   },
   "source": [
    "## Discussion and Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GsEYB7isA35h"
   },
   "source": [
    "### Discussion on the CNN Results\n",
    "The optimized model unfortunately had to be cut short, as 48 hours in, we had around 9 CNNs that had run with a selection of different hyperparameters. Some models had a steadily decreasing loss which indicates that the models were learning at an okay rate. Due to the model not being able to test all parameters, we don’t have a full set of results to analyze, and we don’t have all of our assessment parameters to work with either\n",
    "\n",
    "In the future, this optimization process will have all assessment feature (precision, recall, f1-score and accuracy) implemented in order to analyze the model completely In addition to this, the model would ideally be run on the images in a larger, more interpretable resolution. Not to mention, the selection of hyperparameters will all be evaluated, even if it takes multiple days to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5FV7AxGoA35h"
   },
   "source": [
    "## Author contribution\n",
    "\n",
    "Colby Degan - Responsible for making the SVC\n",
    "\n",
    "Jose Villegas - Responsible for making the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kzWeu0nMA35h"
   },
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EsoyKxQlA35h"
   },
   "source": [
    "SciKit Learn. (2019). sklearn.model_selection.GridSearchCV — scikit-learn 0.22 Documentation. Scikit-Learn.org. https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "\n",
    "Pytorch. PyTorch documentation.  https://pytorch.org/docs/stable/index.html\n",
    "\n",
    "Geeksforgeeks. (2025). Building a Convolutional Neural Network using PyTorch. https://www.geeksforgeeks.org/building-a-convolutional-neural-network-using-pytorch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.11 (default)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
